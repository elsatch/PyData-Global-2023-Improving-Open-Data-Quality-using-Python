{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of single datasets\n",
    "\n",
    "In this notebook we will explore how to validate an individual dataset. We will follow the next steps:\n",
    "\n",
    "- Basic Exploratory Data Analysis\n",
    "- Profile data & identify anomalies with Great Expectations\n",
    "- Fixing problems detected using pandas\n",
    "- Assesing data quality improvements after remediation steps were taken\n",
    "\n",
    "To do so, we will use the datasets available in the `data` folder. These datasets are:\n",
    "\n",
    "- `data/a.csv`: Data about traffic accidents from city of Toronto. Website: \n",
    "- `data/b.csv`: Data about traffic accidents from city of Toronto. Website:\n",
    "- `data/c.csv`: Data about traffic accidents from city of Toronto. Website:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment\n",
    "\n",
    "First, we should create a python virtual environment and install the required dependencies. To do so, we can run the following commands:\n",
    "\n",
    "```bash\n",
    "python -m venv data-quality\n",
    "```\n",
    "Now depending on your OS, you should run the following command:\n",
    "\n",
    "- Linux/MacOS\n",
    "\n",
    "```bash\n",
    "source data-quality/bin/activate\n",
    "```\n",
    "- Windows\n",
    "\n",
    "```PowerShell\n",
    "data-quality\\Scripts\\Activate.ps1\n",
    "```\n",
    "\n",
    "Finally, we can install the required dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "XXX Fix colab XXX\n",
    "\n",
    "You can also launch the notebook in colab, clicking the following button: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Exploratory Data Analysis\n",
    "\n",
    "First step in our analysis is to perform a basic exploratory data analysis. One simple library that provides useful features is SweetViz. Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas and sweetviz\n",
    "\n",
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "import os\n",
    "\n",
    "\n",
    "# load the dataset into a dataframe\n",
    "\n",
    "bikes_file = os.path.join('data', 'input', 'bikes','bicycle-thefts - 4326.csv')\n",
    "bikes_df = pd.read_csv(bikes_file)\n",
    "bikes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic pandas exploration\n",
    "bikes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EDA using sweetviz\n",
    "\n",
    "bikes_report = sv.analyze(bikes_df)\n",
    "bikes_report.show_html('bikes_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review dataset information\n",
    "\n",
    "There are some columns that look quite broken, like the missing DATES. Let's expand our search to other documents.\n",
    "\n",
    "To explore the domain of this data, we can visit the following links:\n",
    "\n",
    "- [Explore Bicycle Thefts Open Data](https://data.torontopolice.on.ca/datasets/TorontoPS::bicycle-thefts-open-data/explore)\n",
    "- [About Bicycle Thefts Open Data](https://data.torontopolice.on.ca/datasets/TorontoPS::bicycle-thefts-open-data/about)\n",
    "- [Public Safety Open Data Documentation](https://ago-item-storage.s3.amazonaws.com/c0b17f1888544078bf650f3b8b04d35d/PSDP_Open_Data_Documentation.pdf)\n",
    "- [Bicycle Data Code Sheet](https://ago-item-storage.s3.amazonaws.com/332f6e958b704d9aa023e7c3487f710f/Bicycle_Data_Code_Sheet.pdf)\n",
    "\n",
    "Things we can learn from the documentation:\n",
    "\n",
    "\"In accordance with the Municipal Freedom of Information and Protection of Privacy Act, the Toronto Police Service has taken the necessary measures to protect the privacy of individuals involved in the reported occurrences. **No personal information related to any of the parties involved in the occurrence will be released as open data.**\"\n",
    "\n",
    "The downloadable datasets display the REPORT_DATE and OCC_DATE fields in UTC timezone.\n",
    "\n",
    "Latest update: 2023-10-07\n",
    "\n",
    "Latest Open Data Portal update: 2023-11-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our quality requirements\n",
    "\n",
    "From the dataset description we can derive the following information:\n",
    "\n",
    "- Location has not been able to be verified -> Coordinates may be blank or outside City of Toronto\n",
    "- Fields have been included for both the old 140 City of Toronto Neighbourhoods structure as well as the new 158 City of Toronto Neighbourhoods structure\n",
    "- This dataset contains Bicycle Thefts occurrences from 2014-2020.\n",
    "- The location of crime occurrences have been deliberately offset to the nearest road intersection node to protect the privacy of parties involved in the occurrence. Due to the offset of occurrence location, the numbers by Division and Neighbourhood may not reflect the exact count of occurrences reported within these geographies\n",
    "- Toronto Police Service does not guarantee the accuracy, completeness, timeliness of the data and it should not be compared to any other source of crime data\n",
    "- There is no personal data included in this dataset, so we don't have to be extra careful with PII data.\n",
    "\n",
    "In the Public Safety site, they claim the dataset covers bike thefts from 2014 to 2022, instead of 2020.\n",
    "\n",
    "There is a site with data visualizations! https://data.torontopolice.on.ca/pages/bicycle-thefts\n",
    "\n",
    "According to the Bicycle Thefts Dashboard: \"Statistics are updated annually using the prepared open dataset provided for download. Current Year-to-Date data is not available for download.\" But... Current Year-to-Date data is visible in the Dashbord!\n",
    "\n",
    "Let's explore data a bit:\n",
    "\n",
    "- Historical data is available from 2014 to 2022. Yet, if we check the total numbers in the dashboard, the number of total thelfts is 31.972. In the dataset we have downloaded, we have 34.290 registered thefts. There is a difference of 2.318 registers (around 7% of the total)\n",
    "\n",
    "There is another map, that covers all crime: https://torontops.maps.arcgis.com/apps/webappviewer/index.html?id=300d35778c114ef49d59454225043681\n",
    "\n",
    "The documentation reports the following fields:\n",
    "\n",
    "| Field | Field Name           | Description |\n",
    "|-------|----------------------|-------------|\n",
    "| 1     | EVENT_UNIQUE_ID      | Offence Number |\n",
    "| 2     | PRIMARY_OFFENCE      | Primary Offence Type |\n",
    "| 3     | OCC_DATE             | Date Offence Occurred (time is displayed in UTC format when downloaded as a CSV) |\n",
    "| 4     | OCC_YEAR             | Year Offence Occurred |\n",
    "| 5     | OCC_MONTH            | Month Offence Occurred |\n",
    "| 6     | OCC_DOW              | Day of the Week Offence Occurred |\n",
    "| 7     | OCC_DAY              | Day of the Month Offence Occurred |\n",
    "| 8     | OCC_DOY              | Day of the Year Offence Occurred |\n",
    "| 9     | OCC_HOUR             | Hour Offence Occurred |\n",
    "| 10    | REPORT_DATE          | Date Offence was Reported (time is displayed in UTC format when downloaded as a CSV) |\n",
    "| 11    | REPORT_YEAR          | Year Offence was Reported |\n",
    "| 12    | REPORT_MONTH         | Month Offence was Reported |\n",
    "| 13    | REPORT_DOW           | Day of the Week Offence was Reported |\n",
    "| 14    | REPORT_DAY           | Day of the Month Offence was Reported |\n",
    "| 15    | REPORT_DOY           | Day of the Year Offence was Reported |\n",
    "| 16    | REPORT_HOUR          | Hour Offence was Reported |\n",
    "| 17    | DIVISION             | Police Division where Offence Occurred |\n",
    "| 18    | LOCATION_TYPE        | Location Type of Offence |\n",
    "| 19    | PREMISES_TYPE        | Premises Type of Offence |\n",
    "| 20    | BIKE_MAKE            | Make of Bicycle |\n",
    "| 21    | BIKE_MODEL           | Model of Bicycle |\n",
    "| 22    | BIKE_TYPE            | Type of Bicycle |\n",
    "| 23    | BIKE_SPEED           | Speed of Bicycle |\n",
    "| 24    | BIKE_COLOUR          | Colour of Bicycle |\n",
    "| 25    | BIKE_COST            | Cost of Bicycle |\n",
    "| 26    | STATUS               | Status of Bicycle |\n",
    "| 27    | HOOD_158             | Identifier of Neighbourhood using City of Toronto's new 158 neighbourhood structure |\n",
    "| 28    | NEIGHBOURHOOD_158    | Name of Neighbourhood using City of Toronto's new 158 neighbourhood structure |\n",
    "| 29    | HOOD_140             | Identifier of Neighbourhood using City of Toronto's old 140 neighbourhood structure |\n",
    "| 30    | NEIGHBOURHOOD_140    | Name of Neighbourhood using City of Toronto's old 140 neighbourhood structure |\n",
    "| 31    | LONG_WGS84           | Longitude Coordinates (Offset to nearest intersection) |\n",
    "\n",
    "\n",
    "Our dataset presents the following columns:\n",
    "\n",
    "| Number | Column           | Description |\n",
    "|--------|------------------|-------------|\n",
    "| 0      | _id              | Unique row identifier for Open Data database |\n",
    "| 1      | EVENT_UNIQUE_ID  | Offence Number |\n",
    "| 2      | PRIMARY_OFFENCE  | Primary Offence Type |\n",
    "| 3      | OCC_DATE         | Date of Offence |\n",
    "| 4      | OCC_YEAR         | Year Offence Occurred |\n",
    "| 5      | OCC_MONTH        | Month Offence Occurred |\n",
    "| 6      | OCC_DOW          | Day of the Month Offence Occurred |\n",
    "| 7      | OCC_DAY          | Day of the Year Offence Occurred |\n",
    "| 8      | OCC_DOY          | Day of the Week Offence Occurred |\n",
    "| 9      | OCC_HOUR         | Hour Offence Occurred |\n",
    "| 10     | REPORT_DATE      | Date Offence was Reported |\n",
    "| 11     | REPORT_YEAR      | Year Offence was Reported |\n",
    "| 12     | REPORT_MONTH     | Month Offence was Reported |\n",
    "| 13     | REPORT_DOW       | Day of the Month Offence was Reported |\n",
    "| 14     | REPORT_DAY       | Day of the Year Offence was Reported |\n",
    "| 15     | REPORT_DOY       | Day of the Week Offence was Reported |\n",
    "| 16     | REPORT_HOUR      | Hour Offence was Reported |\n",
    "| 17     | DIVISION         | Police Division where Offence Occurred |\n",
    "| 18     | LOCATION_TYPE    | Location Type of Offence |\n",
    "| 19     | PREMISES_TYPE    | Premises Type of Offence |\n",
    "| 20     | BIKE_MAKE        | Make of Bicycle |\n",
    "| 21     | BIKE_MODEL       | Model of Bicycle |\n",
    "| 22     | BIKE_TYPE        | Type of Bicycle |\n",
    "| 23     | BIKE_SPEED       | Speed of Bicycle |\n",
    "| 24     | BIKE_COLOUR      | Colour of Bicycle |\n",
    "| 25     | BIKE_COST        | Cost of Bicycle |\n",
    "| 26     | STATUS           | Status of Bicycle |\n",
    "| 27     | geometry         |             |\n",
    "\n",
    "What are the differences between those tables? Let's check it out (helped by GPT-4):\n",
    "\n",
    "| Field | Original Description                                    | New Description                                   | Difference |\n",
    "|-------|---------------------------------------------------------|---------------------------------------------------|------------|\n",
    "| 0     | *Not Present*                                           | Unique row identifier for Open Data database     | Added in new data |\n",
    "| 1     | Offence Number                                          | Offence Number                                    | Same |\n",
    "| 2     | Primary Offence Type                                    | Primary Offence Type                              | Same |\n",
    "| 3     | Date Offence Occurred (UTC format in CSV)               | Date of Offence                                   | Description detail |\n",
    "| 4     | Year Offence Occurred                                   | Year Offence Occurred                             | Same |\n",
    "| 5     | Month Offence Occurred                                  | Month Offence Occurred                            | Same |\n",
    "| 6     | Day of the Week Offence Occurred                        | Day of the Month Offence Occurred                 | Different |\n",
    "| 7     | Day of the Month Offence Occurred                       | Day of the Year Offence Occurred                  | Different |\n",
    "| 8     | Day of the Year Offence Occurred                        | Day of the Week Offence Occurred                  | Different |\n",
    "| 9     | Hour Offence Occurred                                   | Hour Offence Occurred                             | Same |\n",
    "| ...   | ...                                                     | ...                                               | ... |\n",
    "| 27    | Identifier of Neighbourhood using City of Toronto's...  | *Not Present*                                     | Removed in new data |\n",
    "| 28    | Name of Neighbourhood using City of Toronto's...        | *Not Present*                                     | Removed in new data |\n",
    "| 29    | Identifier of Neighbourhood using City of Toronto's...  | *Not Present*                                     | Removed in new data |\n",
    "| 30    | Name of Neighbourhood using City of Toronto's...        | *Not Present*                                     | Removed in new data |\n",
    "| 31    | Longitude Coordinates (Offset to nearest intersection)  | *Not Present*                                     | Removed in new data |\n",
    "| 27    | *Not Present*                                           | Geometry                                          | Added in new data |\n",
    "\n",
    "Yes, the original column descriptions in the dataset are flipped out!\n",
    "\n",
    "![Original Data Features](images\\data_features.png)\n",
    "\n",
    "But there is more... there is another dataset, more complete dataset in the Public Safety site!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's explore the alternative dataset!\n",
    "\n",
    "There is no Neighbourhood information in the Open Data site dataset! The dates are mostrly missing and from medieval times! Let's see if both issues are fixed in the alternative dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's repeat the process for the Bicycle_Thefts_Open_Data_Public_Safety_version.csv\n",
    "\n",
    "bikes_ps_file = os.path.join('data','input','bikes','Bicycle_Thefts_Open_Data_Public_Safety_version.csv')\n",
    "bikes_ps_df = pd.read_csv(bikes_ps_file)\n",
    "bikes_ps_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neighboorhood information is back! The dates look correct in ISO 8601 format but... we have got new colums X, Y with another coordinates! These do not appear in the documentation (But happen to be Web Mercator coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the sweetviz report for this new dataframe\n",
    "\n",
    "bikes_ps_report = sv.analyze(bikes_ps_df)\n",
    "bikes_ps_report.show_html('bikes_ps_report.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "There are almost no missing values in this dataset. Compare this to the original dataset!\n",
    "\n",
    "The data that is not meant to be in the dataset (current year - 2023) is in the dataset, with 2257 entries. There are thefts reported from 1975 and 1983! Also, there about 50 entries prior to 2014 (which is the year the dataset is supposed to start).\n",
    "\n",
    "Most of thefts are reported in OCC_DATE at 04:00:00. Maybe this is the time the pubs close? Are most of these happen during summer months? (Maybe this is after special events: https://bartenderatlas.com/features/travel/how-to-drink-in-toronto/)\n",
    "\n",
    "Division has an NSA value. It is not documented in the guide.\n",
    "\n",
    "Bike Make has 1% missing, but there are UNKNOWN MAKE and UNKNOWN in the dataset. Also, there is a mix of brands and shortcodes. This might make hard to analyze the data.\n",
    "\n",
    "Bike Model has 38% of the data missing. There are UNKNOWN values in the dataset plus some brands used as model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we want to focus on now?\n",
    "\n",
    "In the new file:\n",
    "\n",
    "- Why do we have duplicated Event_Unique_ID? (There are 2.257 duplicated values)\n",
    "- OCC_DATES always show 04:00:00 as time, but contradicts the OCC_HOUR column. We might need to fix the OCC_DATE\n",
    "- Let's make sure we have the correct values for Division (Divisions should start with the letter D or be NSA)\n",
    "- Let's replace the Bike Brand shortcodes with the brand name\n",
    "- Let's check if the years includded in the dataset description match the years in the dataset\n",
    "- Let's check if the dates are in the correct format\n",
    "\n",
    "Extra\n",
    "- Fix the date in our original example. Date columns should have dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ooc_hours column using matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(bikes_ps_df['OCC_HOUR'], bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the rows that have EVENT_UNIQUE_ID = GO-20201550944\n",
    "\n",
    "bikes_ps_df[bikes_ps_df['EVENT_UNIQUE_ID'] == 'GO-20201550944']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupe_bikes = bikes_ps_df[bikes_ps_df['EVENT_UNIQUE_ID'] == 'GO-20201550944']\n",
    "dupe_bikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting our requirements into Great Expectations\n",
    "\n",
    "We could start by visiting the GX gallery. But you must not do that! First, we need to plan and define our requirements. Let's do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Data Quality cycle\n",
    "\n",
    "PDCA (Plan -> Do -> Check -> Act)\n",
    "\n",
    "#### Plan\n",
    "\n",
    "Let's review our observations:\n",
    "\n",
    "- Event_Unique_ID has duplicate values (There are 2.257 duplicated values)\n",
    "- OCC_DATE: most times show 04:00:00 as hour, but contradicts the OCC_HOUR column. \n",
    "- Districts: should be either NSA or starting by D\n",
    "- Bike Brand should be either shortcodes or brand name, not a mixture of both\n",
    "- Years should be in the range 2014-2023\n",
    "- Dates should be in the correct format\n",
    "\n",
    "Now let's sort them out based on inherent data quality dimensions: Accuracy, Completeness, Consistency, Credibility, Currentness.\n",
    "\n",
    "- Accuracy: Dates should be in the correct format, bike brands should be either shortcodes or brand name, not a mixture of both\n",
    "- Completeness: Years should be in the range 2014-2023\n",
    "- Consistency: Districts should be either NSA or starting by D\n",
    "- Currentness: Years should be in the range 2014-2023\n",
    "- Credibility: no issues\n",
    "\n",
    "Time to go back to slides before browsing the expectations gallery!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping the GX environment using fluent datasources\n",
    "\n",
    "import great_expectations as gx\n",
    "\n",
    "data_folder = os.path.join('data','input', 'bikes')\n",
    "\n",
    "# Setting up a data context\n",
    "\n",
    "context = gx.get_context()\n",
    "\n",
    "datasource_name = \"toronto_bike_thefts\"\n",
    "path_to_folder_containing_csv_files = data_folder\n",
    "\n",
    "datasource = context.sources.add_pandas_filesystem(\n",
    "    name=datasource_name, base_directory=path_to_folder_containing_csv_files\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data asset from the datasource (folder might contain one or multiple files)\n",
    "\n",
    "bike_thefts_data_asset = datasource.add_csv_asset(\n",
    "    name = \"original_bike_thefts_2014_2023\",\n",
    "    batching_regex = \"bicycle-thefts - 4326\\.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch request to recover all the contents of the given file\n",
    "\n",
    "bike_thefts_batch_request = bike_thefts_data_asset.build_batch_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an expectation suite\n",
    "\n",
    "expectation_suite_name = \"bike_thefts_expectations\"\n",
    "\n",
    "expectation_suite = context.add_or_update_expectation_suite(\n",
    "    expectation_suite_name = expectation_suite_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a validator, that allows us to interactively compare the data in the batch with the expectations in the suite\n",
    "\n",
    "validator = context.get_validator(\n",
    "    batch_request=bike_thefts_batch_request,\n",
    "    expectation_suite_name=expectation_suite_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert our assertions into expectations\n",
    "# Event_Unique_ID should have no duplicate values\n",
    "# https://greatexpectations.io/expectations/expect_column_values_to_be_unique?filterType=Backend%20support&gotoPage=1&showFilters=true&viewType=Summary\n",
    "\n",
    "validator.expect_column_values_to_be_unique(column='EVENT_UNIQUE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we explore using a validator, we can update the expectations. If we save the expectations to the expectation suite, the last version for each element will be saved.\n",
    "\n",
    "validator.expect_column_values_to_be_unique(column='EVENT_UNIQUE_ID', mostly=0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if the values in the DIVISION column start with a D\n",
    "validator.expect_column_values_to_match_regex(column='DIVISION',regex='^D.*', meta={\"Characteristic\": \"Accuracy\", \"QM\": \"Syntactic Accuracy\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# District names should be either the values NSA or start by D\n",
    "\n",
    "validator.expect_column_values_to_match_regex(column='DIVISION',regex='NSA|^D.*', meta={\"Characteristic\": \"Accuracy\", \"QM\": \"Syntactic Accuracy\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other alternative regexps\n",
    "# Regex to match 'D' followed by any numerical characters: ^D\\d+\n",
    "# Regex that matches 'D' followed by 1 or 2 numbers: ^D\\d{1,2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete bike maker names must be incluided in the column BIKE_MAKE\n",
    "\n",
    "# We can extract the list from the Bicycle Code Sheet\n",
    "\n",
    "bike_makers = ['AQUILA',\n",
    " 'ARGON 18',\n",
    " 'BARLETTA',\n",
    " 'BASSO',\n",
    " 'BIANCHI',\n",
    " 'BRODIE',\n",
    " 'CANNONDALE',\n",
    " 'CARRERA',\n",
    " 'CCM',\n",
    " 'CERVELO',\n",
    " 'COLNAGO',\n",
    " 'CUBE',\n",
    " 'DEVINCI',\n",
    " 'DIAMONDBACK',\n",
    " 'ELECTRA',\n",
    " 'EMMO',\n",
    " 'FELT',\n",
    " 'FUJI',\n",
    " 'GARY FISHER',\n",
    " 'GIANT',\n",
    " 'GT',\n",
    " 'HARO',\n",
    " 'HUFFY',\n",
    " 'HUSKY',\n",
    " 'INFINITY',\n",
    " 'IRON HORSE',\n",
    " 'JAMIS',\n",
    " 'KHS',\n",
    " 'KLEIN',\n",
    " 'KONA',\n",
    " 'LAPIERRE',\n",
    " 'LEMOND',\n",
    " 'LITE SPEED',\n",
    " 'LOUIS GARNEAU',\n",
    " 'MARIN OR MARINONI',\n",
    " 'MERIDA',\n",
    " 'MIELE',\n",
    " 'MIKADO',\n",
    " 'MILLENNIUM',\n",
    " 'MONGOOSE',\n",
    " 'NAKAMURA',\n",
    " 'NEXT',\n",
    " 'NORCO',\n",
    " 'OPUS',\n",
    " 'ORYX',\n",
    " 'OTHER',\n",
    " 'PEUGEOT',\n",
    " 'PINARELLO',\n",
    " 'PIVOT',\n",
    " 'PROTOUR',\n",
    " 'RALEIGH',\n",
    " 'REDLINE',\n",
    " 'ROCKY MOUNTAIN',\n",
    " 'SANTA CRUZ',\n",
    " 'SCHWINN',\n",
    " 'SCOTT',\n",
    " 'SPECIALIZED',\n",
    " 'SPORTEK',\n",
    " 'SUPERCYCLE',\n",
    " 'SUN',\n",
    " 'SURLY',\n",
    " 'TECH TEAM',\n",
    " 'THIN BLUE LINE',\n",
    " 'TREK',\n",
    " 'TRUE NORTH',\n",
    " 'TURCO',\n",
    " 'VAGABOND',\n",
    " 'VELO SPORT',\n",
    " 'YETI',\n",
    " 'UNKNOWN MAKE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_dict = {'AQUI': 'AQUILA',\n",
    " 'ARGO': 'ARGON 18',\n",
    " 'BARL': 'BARLETTA',\n",
    " 'BASS': 'BASSO',\n",
    " 'BIAN': 'BIANCHI',\n",
    " 'BROD': 'BRODIE',\n",
    " 'CANN': 'CANNONDALE',\n",
    " 'CARR': 'CARRERA',\n",
    " 'CC': 'CCM',\n",
    " 'CERV': 'CERVELO',\n",
    " 'COLN': 'COLNAGO',\n",
    " 'CUBE': 'CUBE',\n",
    " 'DEVI': 'DEVINCI',\n",
    " 'DIAM': 'DIAMONDBACK',\n",
    " 'ELEC': 'ELECTRA',\n",
    " 'EM': 'EMMO',\n",
    " 'FELT': 'FELT',\n",
    " 'FUJI': 'FUJI',\n",
    " 'GARY': 'GARY FISHER',\n",
    " 'GI': 'GIANT',\n",
    " 'GT': 'GT',\n",
    " 'HARO': 'HARO',\n",
    " 'HUFF': 'HUFFY',\n",
    " 'HUSK': 'HUSKY',\n",
    " 'INFI': 'INFINITY',\n",
    " 'IRON': 'IRON HORSE',\n",
    " 'JAMI': 'JAMIS',\n",
    " 'KHS': 'KHS',\n",
    " 'KLEI': 'KLEIN',\n",
    " 'KONA': 'KONA',\n",
    " 'LAPI': 'LAPIERRE',\n",
    " 'LEMO': 'LEMOND',\n",
    " 'LITE': 'LITE SPEED',\n",
    " 'LOUI': 'LOUIS GARNEAU',\n",
    " 'MARI': 'MARIN OR MARINONI',\n",
    " 'MERI': 'MERIDA',\n",
    " 'MIEL': 'MIELE',\n",
    " 'MIKA': 'MIKADO',\n",
    " 'MILL': 'MILLENNIUM',\n",
    " 'MONG': 'MONGOOSE',\n",
    " 'NAKA': 'NAKAMURA',\n",
    " 'NEXT': 'NEXT',\n",
    " 'NO': 'NORCO',\n",
    " 'OPUS': 'OPUS',\n",
    " 'ORYX': 'ORYX',\n",
    " 'OT': 'OTHER',\n",
    " 'PEUG': 'PEUGEOT',\n",
    " 'PINA': 'PINARELLO',\n",
    " 'PIVO': 'PIVOT',\n",
    " 'PROT': 'PROTOUR',\n",
    " 'RA': 'RALEIGH',\n",
    " 'REDL': 'REDLINE',\n",
    " 'ROCK': 'ROCKY MOUNTAIN',\n",
    " 'SANT': 'SANTA CRUZ',\n",
    " 'SC': 'SCHWINN',\n",
    " 'SCOT': 'SCOTT',\n",
    " 'SE': 'SPECIALIZED',\n",
    " 'SPOR': 'SPORTEK',\n",
    " 'SU': 'SUPERCYCLE',\n",
    " 'SUN': 'SUN',\n",
    " 'SURL': 'SURLY',\n",
    " 'TECH': 'TECH TEAM',\n",
    " 'THIN': 'THIN BLUE LINE',\n",
    " 'TR': 'TREK',\n",
    " 'TRUE': 'TRUE NORTH',\n",
    " 'TURC': 'TURCO',\n",
    " 'VAGA': 'VAGABOND',\n",
    " 'VELO': 'VELO SPORT',\n",
    " 'YETI': 'YETI',\n",
    " 'UNKNOWN': 'UNKNOWN MAKE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will check the contents of BIKE_MAKE against our list of complete bike maker names using the expectation expect_column_values_to_be_in_set\n",
    "\n",
    "validator.expect_column_values_to_be_in_set(column='BIKE_MAKE', value_set=bike_makers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will check the contents of BIKE_MAKE against the short codes using the expectation expect_column_values_to_be_in_set and the keys in the bike_dict\n",
    "\n",
    "bike_codes = list(bike_dict.keys())\n",
    "\n",
    "validator.expect_column_values_to_be_in_set(column='BIKE_MAKE', value_set=bike_codes, meta={\"Characteristic\": \"XXX\", \"QM\": \"XXX\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's detect values that are not in the code sheet either as codes or brands\n",
    "\n",
    "merged_list = bike_makers + bike_codes\n",
    "\n",
    "# We eliminate duplicate values\n",
    "merged_list_unique = list(set(merged_list))\n",
    "\n",
    "validator.expect_column_values_to_be_in_set(column='BIKE_MAKE', value_set=merged_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dates should be in the correct format\n",
    "\n",
    "validator.expect_column_values_to_be_dateutil_parseable(column='OCC_DATE', meta={\"Characteristic\": \"Accuracy\", \"QM\": \"Syntactic Accuracy\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_values_to_be_dateutil_parseable(column='REPORT_DATE', meta={\"Characteristic\": \"Accuracy\", \"QM\": \"Syntactic Accuracy\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expecting that years should be in the range 2014-2023\n",
    "validator.expect_column_values_to_be_between(column='OCC_YEAR', min_value=2014, max_value=2023, meta={\"Characteristic\": \"Completeness\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's be a little bit less strict and allow for 2013 as well\n",
    "validator.expect_column_values_to_be_between(column='OCC_YEAR', min_value=2013, max_value=2023 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the REPORT_YEAR column now\n",
    "\n",
    "validator.expect_column_values_to_be_between(column='REPORT_YEAR', min_value=2013, max_value=2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So someone reported in 2023 that their bike was stolen in 1975! In all previous descriptions, about dates, we should change dates by reported dates. It doesn't matter when the bike was stolen, but when it was reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure that no empty values are allowed in the OCC_DATE or REPORT_DATE columns\n",
    "\n",
    "validator.expect_column_values_to_not_be_null(column='OCC_DATE')\n",
    "validator.expect_column_values_to_not_be_null(column='REPORT_DATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Can you find other alternative expectations that could be tested against dates? (Hint: Visit the gallery and report back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a checkpoint to store all the data related to expectation validation\n",
    "\n",
    "checkpoint_pre_fixes = gx.checkpoint.SimpleCheckpoint(\n",
    "    name=\"checkpoint_bike_theft_pre_fixes\",\n",
    "    data_context=context,\n",
    "    validations=[\n",
    "        {\n",
    "            \"batch_request\": bike_thefts_batch_request,\n",
    "            \"expectation_suite_name\": expectation_suite_name,\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we save our expectations\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run the checkpoint to validate the expectations against the data we extracted from the batch_request\n",
    "\n",
    "checkpoint_result = checkpoint_pre_fixes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And last, but not least, we generate a report using the DataDocs feature from Great Expectations\n",
    "\n",
    "context.build_data_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing problems detected using pandas\n",
    "\n",
    "IMHO Raw data should not be mutable, otherwise we lose track of the changes, the tests, etc. So first step, let's create a copy of the original dataset in the input folder into the interim folder.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a copy of the original dataset in the input folder into the interim folder\n",
    "\n",
    "import shutil\n",
    "\n",
    "shutil.copyfile(os.path.join('data','input', 'bikes', 'bicycle-thefts - 4326.csv'), os.path.join('data','interim', 'bikes', 'bicycle-thefts - 4326.csv'))\n",
    "\n",
    "# Note: We will be working with another dataframe. THIS DOES NOT MODIFY ANY THE DATE WE HAVE ALREADY VALIDATED \n",
    "\n",
    "bike_thefts_df = pd.read_csv(os.path.join('data', 'interim', 'bikes', 'bicycle-thefts - 4326.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform all the changes in the interim folder, modifying the dataset in place. We will perform four fixes and a half (fictional) fix! XXX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 25 most common values in the BIKE_MAKE column\n",
    "\n",
    "bike_thefts_df['BIKE_MAKE'].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIX1: Fixing (partially) the bike brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix 1 - Replace the bike codes with the bike make (longer form is more explicit) using the bike_dict\n",
    "\n",
    "bike_thefts_df['BIKE_MAKE'] = bike_thefts_df['BIKE_MAKE'].replace(bike_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_thefts_df['BIKE_MAKE'].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending of our goal we could perform additional fixes. For example, we could replace all missing values with UNKNOWN MAKE, so we have no missing values. We could also replace all the failing rows with the OTHER brand, as thoese are not tracked in the report. This would make us to lose some details about the brands involved, so it's not such a good idea.\n",
    "\n",
    "The best idea: report back to the source that the Bicycle Code Sheet is incomplete, as it doesn't contain all codified values. Also, ask for a review of the data, as there are brands that look like BIKE_TYPE instead of make like E-BIKE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace missing values with the string 'UNKNOWN MAKE'\n",
    "\n",
    "bike_thefts_df['BIKE_MAKE'] = bike_thefts_df['BIKE_MAKE'].fillna('UNKNOWN MAKE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIX 2: Fixing the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix 2 - Let's recreate the OCC_DATE column using the OCC_YEAR, OCC_MONTH and OCC_DAY, OCC_HOUR columns, producing an ISO 8601 compliant date\n",
    "\n",
    "bike_thefts_df[\"OCC_DATE\"] = (\n",
    "    bike_thefts_df[\"OCC_YEAR\"].astype(str)\n",
    "    + \"-\"\n",
    "    + bike_thefts_df[\"OCC_MONTH\"].astype(str).str.zfill(2)\n",
    "    + \"-\"\n",
    "    + bike_thefts_df[\"OCC_DAY\"].astype(str).str.zfill(2)\n",
    "    + \"T\"\n",
    "    + bike_thefts_df[\"OCC_HOUR\"].astype(str).str.zfill(2)\n",
    "    + \":00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_thefts_df['OCC_DATE'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data, we discover that OCC_MONTH has month names instead of month numbers, so we are not able to perform the previous operation. Let's fix it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert the month names into the corresponding month number\n",
    "\n",
    "month_dict = {\n",
    "    '01': 'January',\n",
    "    '02': 'February',\n",
    "    '03': 'March',\n",
    "    '04': 'April',\n",
    "    '05': 'May',\n",
    "    '06': 'June',\n",
    "    '07': 'July',\n",
    "    '08': 'August',\n",
    "    '09': 'September',\n",
    "    '10': 'October',\n",
    "    '11': 'November',\n",
    "    '12': 'December'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We inverse the dictionary \n",
    "month_number_dict = {v: k for k, v in month_dict.items()}\n",
    "\n",
    "bike_thefts_df['OCC_MONTH'] = bike_thefts_df['OCC_MONTH'].map(month_number_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_thefts_df[\"OCC_DATE\"] = (\n",
    "    bike_thefts_df[\"OCC_YEAR\"].astype(str)\n",
    "    + \"-\"\n",
    "    + bike_thefts_df[\"OCC_MONTH\"].astype(str).str.zfill(2)\n",
    "    + \"-\"\n",
    "    + bike_thefts_df[\"OCC_DAY\"].astype(str).str.zfill(2)\n",
    "    + \"T\"\n",
    "    + bike_thefts_df[\"OCC_HOUR\"].astype(str).str.zfill(2)\n",
    "    + \":00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_thefts_df['OCC_DATE'] = pd.to_datetime(bike_thefts_df['OCC_DATE'])\n",
    "bike_thefts_df['OCC_DATE'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will repeat the process for the REPORT_DATE column\n",
    "\n",
    "bike_thefts_df[\"REPORT_MONTH\"] = bike_thefts_df[\"REPORT_MONTH\"].map(month_number_dict)\n",
    "\n",
    "bike_thefts_df[\"REPORT_DATE\"] = (\n",
    "    bike_thefts_df[\"REPORT_YEAR\"].astype(str)\n",
    "    + \"-\"\n",
    "    + bike_thefts_df[\"REPORT_MONTH\"].astype(str).str.zfill(2)\n",
    "    + \"-\"\n",
    "    + bike_thefts_df[\"REPORT_DAY\"].astype(str).str.zfill(2)\n",
    "    + \"T\"\n",
    "    + bike_thefts_df[\"REPORT_HOUR\"].astype(str).str.zfill(2)\n",
    "    + \":00:00\"\n",
    ")\n",
    "\n",
    "bike_thefts_df['REPORT_DATE'] = pd.to_datetime(bike_thefts_df['REPORT_DATE'])\n",
    "\n",
    "bike_thefts_df['REPORT_DATE'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional fixes\n",
    "\n",
    "Those were our fixes for the original file, that had some missing dates. But it has no neighbourhood information. So let's now focus on fixing the alternative file too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a copy of the original alternate dataset and load it as dataframe\n",
    "\n",
    "shutil.copyfile(os.path.join('data','input', 'bikes', 'Bicycle_Thefts_Open_Data_Public_Safety_version.csv'), os.path.join('data','interim', 'bikes', 'Bicycle_Thefts_Open_Data_Public_Safety_version.csv'))\n",
    "\n",
    "bike_ps_df = pd.read_csv(os.path.join('data', 'interim', 'bikes', 'Bicycle_Thefts_Open_Data_Public_Safety_version.csv'))\n",
    "\n",
    "bike_ps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Fix 3 - Spiltting the compound columns into separate columns (NEIGHBOURHOOD_140 column includes the neighbourhood name and the neighbourhood code into the same column\n",
    "# The code is available also as a separate column HOOD_140, so we need to remove it from the column. We will do this using a regex to remove the (nn) part at the end of the string\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_code(text):\n",
    "    return re.sub(r'\\s\\(\\d+\\)', '', text)\n",
    "\n",
    "bike_ps_df['NEIGHBOURHOOD_140'] = bike_ps_df['NEIGHBOURHOOD_140'].apply(remove_code)\n",
    "\n",
    "# The simpler did not work (I assume because of the datatype not being str! bike_ps_df['NEIGHBOURHOOD_140'] = bike_ps_df['NEIGHBOURHOOD_140'].str.replace(r'\\s\\(\\d+\\)', '')\n",
    "\n",
    "bike_ps_df['NEIGHBOURHOOD_140'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real scenario, we would have started with the alternative file, as it has more data. In this tutorial, we wanted to show how to fix data. But now, it's time to extract those columns and merge them with the original file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of bike_ps_df with only the required columns\n",
    "bike_ps_subset = bike_ps_df[['OBJECTID', 'HOOD_158', 'NEIGHBOURHOOD_158', 'HOOD_140', 'NEIGHBOURHOOD_140']]\n",
    "\n",
    "# Merge both dataframe on their primary indexes using a left join\n",
    "\n",
    "merged_df = pd.merge(bike_thefts_df, bike_ps_subset, left_on='_id', right_on='OBJECTID', how='left')\n",
    "\n",
    "# Drop the duplicate index\n",
    "\n",
    "merged_df.drop(columns=['OBJECTID'], inplace=True)\n",
    "\n",
    "# Check the results\n",
    "\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a brief EDA of the merged dataframe using sweetviz again, focusing on the new columns\n",
    "\n",
    "merged_report = sv.analyze(merged_df)\n",
    "merged_report.show_html('merged_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have done all the fixes, it's time to export it as a new file. We will export it to the interim folder, as it's a temporary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save this dataframe as bike_thefts_merged.csv in the interim folder\n",
    "\n",
    "merged_df.to_csv(os.path.join('data', 'interim', 'bikes', 'bike_thefts_merged.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assesing data quality improvements after remediation steps were taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the environment for this new file\n",
    "\n",
    "# Data Context: Done - Value: toronto_bike_thefts\n",
    "# Expectation Suite: Done - Value: bike_thefts_expectations\n",
    "# Data Source: TBD - toronto_bike_thefts_fixed (We are loading the data from the interim folder instead of the input folder. It is a different folder (and also includes many files))\n",
    "# Data Asset: TBD - bike_thefts_fixed_v1 (We need to explicitely pass the file name bike_thefts_merged.csv)\n",
    "# Batch Request: TBD - batch_request_bike_thefts_fixed_v1 - We will load all the registers from the new file)\n",
    "# Checkpoint: TBD - checkpoint_bike_theft_post_fixes_v1\n",
    "\n",
    "# Setting the data source\n",
    "\n",
    "datasource_fixed_name = \"toronto_bike_thefts_fixed\"\n",
    "path_to_folder_containing_fixed_csv_files = os.path.join('data','interim', 'bikes')\n",
    "\n",
    "datasource_fixed = context.sources.add_pandas_filesystem(\n",
    "    name=datasource_fixed_name, base_directory=path_to_folder_containing_fixed_csv_files\n",
    ")\n",
    "\n",
    "# Creating the data asset\n",
    "\n",
    "bike_thefts_fixed_data_asset = datasource_fixed.add_csv_asset(\n",
    "    name = \"bike_thefts_fixed_v1\",\n",
    "    batching_regex = \"bike_thefts_merged\\.csv\",\n",
    ")\n",
    "\n",
    "# Generate a batch request to recover all the contents of the given file\n",
    "\n",
    "batch_request_bike_thefts_fixed_v1 = bike_thefts_fixed_data_asset.build_batch_request()\n",
    "\n",
    "# Generate a checkpoint\n",
    "\n",
    "checkpoint_post_fixes_v1 = gx.checkpoint.SimpleCheckpoint(\n",
    "    name=\"checkpoint_bike_theft_post_fixes_v1\",\n",
    "    data_context=context,\n",
    "    validations=[\n",
    "        {\n",
    "            \"batch_request\": batch_request_bike_thefts_fixed_v1,\n",
    "            \"expectation_suite_name\": expectation_suite_name,\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run the checkpoint to validate the original expectation suite against the data we extracted from the new batch_request\n",
    "\n",
    "checkpoint_post_fixes_v1_result = checkpoint_post_fixes_v1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And last, but not least, we generate a report using the DataDocs feature from Great Expectations\n",
    "\n",
    "context.build_data_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we learned?\n",
    "\n",
    "Quality is a continuous process. Reports will help you keep track of the improvements but you might not get a 100% quality datasets. You can bend the expectations or modify the data, depending on your needs. Think also about what how do you plan to integrate it, visualize it, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
